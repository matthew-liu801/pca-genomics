{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import msprime\n",
    "from scipy import linalg, stats\n",
    "import utils\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import pysam\n",
    "import pandas as pd\n",
    "from typing import Callable\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate data and clean real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean old data\n",
    "\n",
    "vcf_path = \"raw_data/ALL.chr21.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz\"\n",
    "vcf_file = pysam.VariantFile(vcf_path)\n",
    "\n",
    "records = [record for record in vcf_file.fetch()]\n",
    "\n",
    "\n",
    "# Filter data\n",
    "\n",
    "#16 minutes: 154k observations\n",
    "filtered_by_maf = utils.filter_maf(records)\n",
    "#2 minutes: no change\n",
    "filtered_by_missingness = utils.filter_missingness(filtered_by_maf)\n",
    "#13 seconds: no change\n",
    "filtered_by_quality = utils.filter_quality(filtered_by_missingness)\n",
    "\n",
    "records = filtered_by_quality\n",
    "\n",
    "# Getting the number of samples and SNPs\n",
    "num_samples = len(vcf_file.header.samples)\n",
    "num_snps = len(records)\n",
    "\n",
    "# Create an empty genotype matrix\n",
    "G = np.empty((num_samples, num_snps), dtype=int)\n",
    "\n",
    "# Fill the matrix\n",
    "for j, record in enumerate(records):\n",
    "    for i, sample in enumerate(vcf_file.header.samples):\n",
    "        genotype = record.samples[sample].allele_indices\n",
    "        # Biallelic SNPs\n",
    "        if genotype == (0, 0):\n",
    "            G[i][j] = 0\n",
    "        elif genotype in [(0, 1), (1, 0)]:\n",
    "            G[i][j] = 1\n",
    "        else:\n",
    "            G[i][j] = 2\n",
    "\n",
    "\n",
    "U, S, Vt = np.linalg.svd(G, full_matrices = False)\n",
    "\n",
    "np.savez_compressed(\"../processed_data/real/ALL.chr21.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes)_SVD.npz\", U = U, S = S, Vt = Vt)\n",
    "np.savez_compressed(\"../processed_data/real/ALL.chr21.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.npz\", G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We simulate data of various sizes, sequence lengths, and sparsities\n",
    "sample_sizes = [50, 200, 1000, 10000]\n",
    "sequence_lengths = [10**x for x in range(4, 8)]\n",
    "mutation_rates = [1e-4, 1e-6, 1e-8] #lower mutation rate = sparser\n",
    "recombination_rate = 1e-7\n",
    "\n",
    "G_simulated = {}\n",
    "\n",
    "for n in sample_sizes:\n",
    "    for length in sequence_lengths:\n",
    "        for mut_rate in mutation_rates:\n",
    "            G = utils.simulate_diploid_genotypes(n, length, mut_rate, recombination_rate)\n",
    "            if G.shape[1] >= 10:\n",
    "                G_simulated[(n, length, mut_rate)] = G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save simulated data\n",
    "data_dir = \"../processed_data/simulated\"\n",
    "for (params, G) in G_simulated.items():\n",
    "    file_name = f\"{params[0]}_{params[1]}_{params[2]}.genotypes.npz\"\n",
    "    np.savez_compressed(f\"{data_dir}/{file_name}\", G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real data from 1000 genomes project\n",
    "G = np.load(\"../processed_data/real/ALL.chr21.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.npz\")['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load simulated data\n",
    "\n",
    "sample_sizes = [50, 200, 1000, 10000]\n",
    "sequence_lengths = [10**x for x in range(4, 8)] \n",
    "mutation_rates = [1e-4, 1e-6, 1e-8] #lower mutation rate = sparser\n",
    "recombination_rate = 1e-7\n",
    "\n",
    "G_simulated = {}\n",
    "\n",
    "simulated_dir = \"../processed_data/simulated/\"\n",
    "for n in sample_sizes:\n",
    "    for length in sequence_lengths:\n",
    "        for mut_rate in mutation_rates:\n",
    "            file_name = f\"{n}_{length}_{mut_rate}.genotypes.npz\"\n",
    "            file_dir = simulated_dir + file_name \n",
    "            try:\n",
    "                G = np.load(file_dir)[\"arr_0\"]\n",
    "                G_simulated[(n, length, mut_rate)] = G\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sparsity statistics for G\n",
    "utils.count_genotypes(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sparsity statistics\n",
    "G_simulated_props = {key: utils.count_genotypes(G_simulated[key]) for key in G_simulated.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataframe\n",
    "df_keys = [{'Parameters': key, 'Shape of G': G_simulated[key].shape,\n",
    "         'Proportion Zeros': value[0], 'Proportion Ones': value[1], 'Proportion Twos': value[2]}\n",
    "        for key, value in G_simulated_props.items()]\n",
    "\n",
    "G_df = pd.DataFrame(df_keys)\n",
    "\n",
    "# Write to LaTeX\n",
    "with open('../output/sim_data_table.txt', 'w') as f:\n",
    "    f.write(G_df.to_latex(index=False,\n",
    "                          bold_rows=False,\n",
    "                          float_format=\"%.2f\"))\n",
    "\n",
    "G_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Darnell et al: ARSVD\n",
    "\n",
    "Paper: https://www.jmlr.org/papers/volume18/15-143/15-143.pdf \n",
    "\n",
    "Implementation: https://github.com/gdarnell/arsvd/blob/master/dimension_reduction.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsvd(X, dstar, power_iters=2, delta=10):\n",
    "\t\"\"\" Perform rsvd algorithm on input matrix.\n",
    "\t\tMethod must be supplied dstar.\n",
    "\t\tReturns truncated svd (U,S,V).\n",
    "\tParameters\n",
    "\t----------\n",
    "\tX : int matrix\n",
    "    \tMatrix of n x m integers, where m <= n. If n < m,\n",
    "    \tmatrix will be transposed to enforce m <= n.\n",
    "   \tdstar : int\n",
    "   \t\tThe latent (underlying) matrix rank that will be\n",
    "   \t\tused to truncate the larger dimension (m).\n",
    "   \tpower_iters : int\n",
    "   \t\tdefault: 2\n",
    "   \t\tNumber of power iterations used (random matrix multiplications)\n",
    "   \tdelta : int\n",
    "   \t\tdefault: 10\n",
    "   \t\toversampling parameter (to improve numerical stability)\n",
    "    Returns\n",
    "\t-------\n",
    "\tint matrix\n",
    "    \tMatrix of left singular vectors.\n",
    "    int matrix\n",
    "    \tMatrix of singular values.\n",
    "    int matrix\n",
    "    \tMatrix of right singular vectors.\n",
    "    \"\"\"\n",
    "\ttranspose = False \n",
    "\tif X.shape[0] < X.shape[1]:\n",
    "\t\tX = X.T \n",
    "\t\ttranspose = True \n",
    "\n",
    "\tif power_iters < 1:\n",
    "\t\tpower_iters = 1\n",
    "\n",
    "\t# follows manuscript notation as closely as possible\n",
    "\tP = np.random.randn(X.shape[0],dstar+delta)\t\n",
    "\tfor i in range(power_iters):\n",
    "\t\tP = np.dot(X.T,P)\n",
    "\t\tP = np.dot(X,P)\n",
    "\tQ,R = np.linalg.qr(P)\n",
    "\tB = np.dot(Q.T,X)\n",
    "\tU,S,V = linalg.svd(B)\n",
    "\tU = np.dot(Q, U)\n",
    "\n",
    "\t# Remove extra dimensionality incurred by delta\n",
    "\tU = U[:, 0:dstar]\n",
    "\tS = S[0:dstar]\n",
    "\n",
    "\treturn (V.T, S, U.T) if transpose else (U, S, V)\n",
    "\n",
    "\n",
    "def stabilityMeasure(X, d_max, B=5, power_iters=2):\n",
    "\t\"\"\" Calculate stability of \n",
    "\tParameters\n",
    "\t----------\n",
    "\tX : int matrix\n",
    "\t\tinput matrix to determine rank of\n",
    "\td_max : int\n",
    "\t\tupper bound rank to estimate\n",
    "\tB : int\n",
    "\t\tdefault: 5\n",
    "\t\tnumber of projections to correlate\n",
    "\tpower_iters : int\n",
    "\t\tdefault: 2\n",
    "   \t\tNumber of power iterations used (random matrix multiplications)\n",
    "\tReturns\n",
    "\t-------\n",
    "\tint\n",
    "\t\tLatent (lower-dimensional) matrix rank\n",
    "\t\"\"\"\n",
    "\tsingular_basis = np.zeros((B,X.shape[0],d_max))\n",
    "\t# calculate singular basis under multiple projections\n",
    "\tfor i in range(B):\n",
    "\t\tU = rsvd(X,d_max)[0]\n",
    "\t\tsingular_basis[i,:,:] = U[:,0:d_max]\n",
    "\n",
    "\t# calculate score for each singular vector\n",
    "\tstability_vec = np.zeros((d_max))\n",
    "\tfor k in range(d_max):\n",
    "\t\tstability = 0\n",
    "\t\tfor i in range(0,B-1):\n",
    "\t\t\tfor j in range(i+1,B):\n",
    "\t\t\t\tcorr = stats.spearmanr(singular_basis[i,:,k],singular_basis[j,:,k])[0]\n",
    "\t\t\t\tstability = stability + abs(corr)\n",
    "\t\tN = B*(B-1)/2\n",
    "\t\tstability = stability/N\n",
    "\t\tstability_vec[k] = stability\n",
    "\n",
    "\t# wilcoxon rank-sum test p-values\n",
    "\tp_vals = np.zeros(d_max-2)\n",
    "\tfor k in range(2,d_max):\n",
    "\t\tp_vals[k-2] = stats.ranksums(stability_vec[0:k-1],stability_vec[k-1:d_max])[1]\n",
    "\n",
    "\tdstar = np.argmin(p_vals)\n",
    "\t\n",
    "\treturn dstar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Galinsky et al: FastPCA \n",
    "\n",
    "Paper: https://www.sciencedirect.com/science/article/pii/S0002929716000033\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_X(X: np.array):\n",
    "    \"\"\" \n",
    "    Returns a centered version of X so that each column has mean 0.\n",
    "\n",
    "    Input:\n",
    "    - X (2D numpy array): dataset\n",
    "\n",
    "    Returns:\n",
    "    - Xtilde, centered version of X\n",
    "    \"\"\"\n",
    "    return X - np.mean(X, axis = 0)\n",
    "\n",
    "def normalize_X(X: np.array):\n",
    "    \"\"\" \n",
    "    Normalizes X so each column has variance 1.\n",
    "\n",
    "    Input: \n",
    "    - X (2D numpy array): dataset (assumed to be centered)\n",
    "\n",
    "    Returns: \n",
    "    - Xtilde, normalized version of X\n",
    "    \"\"\"\n",
    "    sd = np.std(X, axis = 0)\n",
    "    return X/sd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fastpca(G, k: int, I: int = 10):\n",
    "    \"\"\" \n",
    "    Our implementation of the FastPCA algorithm in Python. \n",
    "\n",
    "    Inputs:\n",
    "    - G (2D numpy array): the genotype matrix\n",
    "    - k (int): the desired number of principal components\n",
    "    - I (int): the number of iterations\n",
    "\n",
    "    Returns:\n",
    "    - top k principal components\n",
    "    \"\"\"\n",
    "    n, p = G.shape[0], G.shape[1]\n",
    "    # Normalize G\n",
    "    Y = center_X(G)\n",
    "    Y = normalize_X(Y)\n",
    "\n",
    "    # Generate random matrix P0\n",
    "    l = 2 * k # as suggested by the Galinsky paper\n",
    "    P_i = np.random.randn(p, l) #P_0 in the paper; initializing step\n",
    "\n",
    "    H_matrices = []\n",
    "\n",
    "    # Update rule: H_i = Y*P_i, P_{i+1} = 1/n * Y^T * H_i\n",
    "    for _ in range(I):\n",
    "        H_i = Y @ P_i\n",
    "        P_i = 1/n * Y.T @ H_i \n",
    "        H_matrices.append(H_i)\n",
    "\n",
    "    H = np.concatenate(H_matrices, axis = 1)\n",
    "\n",
    "    # Compute SVD of H\n",
    "    U_H, _, _ = np.linalg.svd(H)\n",
    "\n",
    "    T = U_H.T @ Y\n",
    "\n",
    "    _, _, VT_T = np.linalg.svd(T) \n",
    "\n",
    "    return VT_T[:k, :].T    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(f: Callable, k: int, data: dict, *args):\n",
    "    \"\"\" \n",
    "    Benchmarks performance of an algorithm on the given datasets.\n",
    "\n",
    "    Inputs:\n",
    "    - f (Callable): the PCA algorithm\n",
    "        - should take in an argument for data and an argument for number of principal components\n",
    "    - k (int): the desired number of principal components\n",
    "    - data (dict): the datasets, where data[params] = dataset\n",
    "\n",
    "    Returns:\n",
    "    - dict of accuracy metrics by dataset\n",
    "    \"\"\"\n",
    "    accuracy_metrics = {}\n",
    "    for (params, dataset) in data.items():\n",
    "\n",
    "        \"\"\"\n",
    "        # FastPCA times out when attempting to compute an SVD for this matrix\n",
    "            # Takes > 1 hour for just 1 repetition for k values\n",
    "        if dataset.shape in [(1000, 34599), (10000, 3618), (10000, 41561)]:\n",
    "            print(f\"Skipping dataset with shape {dataset.shape} for k = {k}\")\n",
    "            continue \n",
    "        \"\"\"\n",
    "        # Calculate proportion of total variance for top k PCs\n",
    "        accuracy_key = (params, k)\n",
    "        pcs = f(dataset, k, *args)\n",
    "        \n",
    "        accuracy_metrics[accuracy_key] = utils.compute_prop_variance(pcs, dataset)\n",
    "\n",
    "    return accuracy_metrics\n",
    "\n",
    "def benchmark_1000_genomes(f: Callable, k: int, data: np.array, *args):\n",
    "    \"\"\" \n",
    "    Benchmarks performance of an algorithm on the given datasets.\n",
    "\n",
    "    Inputs:\n",
    "    - f (Callable): the PCA algorithm\n",
    "        - should take in an argument for data and an argument for number of principal components\n",
    "    - k (int): the desired number of principal components\n",
    "    - data (2D numpy array): the dataset\n",
    "\n",
    "    Returns:\n",
    "    - accuracy metric\n",
    "    \"\"\"\n",
    "    pcs = f(data, k, *args)\n",
    "    return utils.compute_prop_variance(pcs, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values for k\n",
    "k_vals = [2, 3, 5, 10]\n",
    "\n",
    "num_repetitions = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_arsvd(X, k, dstar):\n",
    "    transpose = False \n",
    "    # Check if transposing is necessary\n",
    "    if X.shape[0] < X.shape[1]:\n",
    "        X = X.T \n",
    "        transpose = True \n",
    "        VT, _, _ = rsvd(X, dstar)\n",
    "    else:\n",
    "        _, _, V = rsvd(X, dstar)\n",
    "\n",
    "    \n",
    "    # Compute top k pcs\n",
    "    Xtilde = center_X(X)\n",
    "\n",
    "    pcs = []\n",
    "\n",
    "    if transpose:\n",
    "        for i in range(min(dstar, k)):\n",
    "            pc = VT[:, i]\n",
    "            pcs.append(pc)\n",
    "    else:\n",
    "        for i in range(min(dstar, k)):\n",
    "            pc = V[i, :]\n",
    "            pcs.append(pc)\n",
    "    \n",
    "    return np.array(pcs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking\n",
    "arsvd_benchmark = []\n",
    "\n",
    "for i in range(num_repetitions):\n",
    "    print(f\"Starting repetition {i + 1}\")\n",
    "    benchmark_totals = []\n",
    "    for k in k_vals:\n",
    "        results = benchmark(pca_arsvd, k, G_simulated, 10)\n",
    "        benchmark_totals.append(results)\n",
    "    arsvd_benchmark.append(benchmark_totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average values across trials \n",
    "arsvd_avgs = {}\n",
    "\n",
    "for i, k in enumerate(k_vals):\n",
    "    for key in arsvd_benchmark[0][i].keys():\n",
    "        avg = (arsvd_benchmark[0][i][key] + arsvd_benchmark[1][i][key])/2\n",
    "        arsvd_avgs[key] = avg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save average values\n",
    "df = pd.DataFrame.from_dict(arsvd_avgs, orient = 'index', columns = ['Value'])\n",
    "\n",
    "# Reset index to separate tuple keys into columns\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'Tuple_Key'}, inplace=True)\n",
    "\n",
    "# Split the tuple keys into separate columns\n",
    "df[['Tuple_1', 'k']] = pd.DataFrame(df['Tuple_Key'].tolist())\n",
    "df[['n', 'sequence_length', 'mut_rate']] = pd.DataFrame(df['Tuple_1'].tolist())\n",
    "\n",
    "# Drop the original Tuple_Key column\n",
    "df.drop(columns=['Tuple_Key', 'Tuple_1'], inplace=True)\n",
    "\n",
    "df = df[['n', 'sequence_length', 'mut_rate', 'k', 'Value']]\n",
    "\n",
    "# Split by value of k\n",
    "df_2 = df[df['k'] == 2]\n",
    "df_3 = df[df['k'] == 3]\n",
    "df_5 = df[df['k'] == 5]\n",
    "df_10 = df[df['k'] == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save values\n",
    "df_2.to_csv(\"../output/arsvd_2.csv\")\n",
    "df_3.to_csv(\"../output/arsvd_3.csv\")\n",
    "df_5.to_csv(\"../output/arsvd_5.csv\")\n",
    "df_10.to_csv(\"../output/arsvd_10.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking\n",
    "fastpca_benchmark = []\n",
    "\n",
    "for i in range(num_repetitions):\n",
    "    print(f\"Starting repetition {i + 1}\") \n",
    "    benchmark_totals = []\n",
    "    for k in k_vals:\n",
    "        print(f\"Starting k = {k}\")\n",
    "        results = benchmark(fastpca, k, G_simulated)\n",
    "        benchmark_totals.append(results)\n",
    "    fastpca_benchmark.append(benchmark_totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average values across trials \n",
    "fastpca_avgs = {}\n",
    "\n",
    "for i, k in enumerate(k_vals):\n",
    "    for key in fastpca_benchmark[0][i].keys():\n",
    "        avg = (fastpca_benchmark[0][i][key] + fastpca_benchmark[1][i][key])/2\n",
    "        fastpca_avgs[key] = avg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save average values\n",
    "df = pd.DataFrame.from_dict(fastpca_avgs, orient = 'index', columns = ['Value'])\n",
    "\n",
    "# Reset index to separate tuple keys into columns\n",
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'Tuple_Key'}, inplace=True)\n",
    "\n",
    "# Split the tuple keys into separate columns\n",
    "df[['Tuple_1', 'k']] = pd.DataFrame(df['Tuple_Key'].tolist())\n",
    "df[['n', 'sequence_length', 'mut_rate']] = pd.DataFrame(df['Tuple_1'].tolist())\n",
    "\n",
    "# Drop the original Tuple_Key column\n",
    "df.drop(columns=['Tuple_Key', 'Tuple_1'], inplace=True)\n",
    "\n",
    "df = df[['n', 'sequence_length', 'mut_rate', 'k', 'Value']]\n",
    "\n",
    "# Split by value of k\n",
    "df_2 = df[df['k'] == 2]\n",
    "df_3 = df[df['k'] == 3]\n",
    "df_5 = df[df['k'] == 5]\n",
    "df_10 = df[df['k'] == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save values\n",
    "df_2.to_csv(\"../output/fastpca_2.csv\")\n",
    "df_3.to_csv(\"../output/fastpca_3.csv\")\n",
    "df_5.to_csv(\"../output/fastpca_5.csv\")\n",
    "df_10.to_csv(\"../output/fastpca_10.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization (Simulated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "arsvd_2 = pd.read_csv(\"../output/arsvd_2.csv\")\n",
    "arsvd_3 = pd.read_csv(\"../output/arsvd_3.csv\")\n",
    "arsvd_5 = pd.read_csv(\"../output/arsvd_5.csv\")\n",
    "arsvd_10 = pd.read_csv(\"../output/arsvd_10.csv\")\n",
    "\n",
    "fastpca_2 = pd.read_csv(\"../output/fastpca_2.csv\")\n",
    "fastpca_3 = pd.read_csv(\"../output/fastpca_3.csv\")\n",
    "fastpca_5 = pd.read_csv(\"../output/fastpca_5.csv\")\n",
    "fastpca_10 = pd.read_csv(\"../output/fastpca_10.csv\")\n",
    "\n",
    "arsvd_2 = arsvd_2.drop(columns = [\"Unnamed: 0\", \"k\"])\n",
    "arsvd_3 = arsvd_3.drop(columns = [\"Unnamed: 0\", \"k\"])\n",
    "arsvd_5 = arsvd_5.drop(columns = [\"Unnamed: 0\", \"k\"])\n",
    "arsvd_10 = arsvd_10.drop(columns = [\"Unnamed: 0\", \"k\"])\n",
    "\n",
    "fastpca_2 = fastpca_2.drop(columns = [\"Unnamed: 0\", \"k\"])\n",
    "fastpca_3 = fastpca_3.drop(columns = [\"Unnamed: 0\", \"k\"])\n",
    "fastpca_5 = fastpca_5.drop(columns = [\"Unnamed: 0\", \"k\"])\n",
    "fastpca_10 = fastpca_10.drop(columns = [\"Unnamed: 0\", \"k\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as tables\n",
    "\n",
    "with open('../output/arsvd_2.txt', 'w') as f:\n",
    "    f.write(arsvd_2.to_latex(index=False,\n",
    "                          bold_rows=False,\n",
    "                          float_format=\"%.2f\"))\n",
    "    \n",
    "\n",
    "with open('../output/arsvd_3.txt', 'w') as f:\n",
    "    f.write(arsvd_3.to_latex(index=False,\n",
    "                          bold_rows=False,\n",
    "                          float_format=\"%.2f\"))\n",
    "\n",
    "\n",
    "with open('../output/arsvd_5.txt', 'w') as f:\n",
    "    f.write(arsvd_5.to_latex(index=False,\n",
    "                          bold_rows=False,\n",
    "                          float_format=\"%.2f\"))\n",
    "\n",
    "\n",
    "with open('../output/arsvd_10.txt', 'w') as f:\n",
    "    f.write(arsvd_10.to_latex(index=False,\n",
    "                          bold_rows=False,\n",
    "                          float_format=\"%.2f\"))\n",
    "\n",
    "\n",
    "with open('../output/fastpca_2.txt', 'w') as f:\n",
    "    f.write(fastpca_2.to_latex(index=False,\n",
    "                          bold_rows=False,\n",
    "                          float_format=\"%.2f\"))\n",
    "    \n",
    "\n",
    "with open('../output/fastpca_3.txt', 'w') as f:\n",
    "    f.write(fastpca_3.to_latex(index=False,\n",
    "                          bold_rows=False,\n",
    "                          float_format=\"%.2f\"))\n",
    "\n",
    "\n",
    "with open('../output/fastpca_5.txt', 'w') as f:\n",
    "    f.write(fastpca_5.to_latex(index=False,\n",
    "                          bold_rows=False,\n",
    "                          float_format=\"%.2f\"))\n",
    "\n",
    "\n",
    "with open('../output/fastpca_10.txt', 'w') as f:\n",
    "    f.write(fastpca_10.to_latex(index=False,\n",
    "                          bold_rows=False,\n",
    "                          float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging performance across all datasets per k\n",
    "arsvd_avgs = [arsvd_2[\"Value\"].mean(), arsvd_3[\"Value\"].mean(), arsvd_5[\"Value\"].mean(), arsvd_10[\"Value\"].mean()]\n",
    "fastpca_avgs = [fastpca_2[\"Value\"].mean(), fastpca_3[\"Value\"].mean(), fastpca_5[\"Value\"].mean(), fastpca_10[\"Value\"].mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averaging performance across all datasets per k, dropping 3 ARSVD datapoints\n",
    "    # Dropping datasets that ARSVD got answers for that FastPCA couldn't\n",
    "    # Indices 16, 20, 22 represent the big datasets we couldn't generate FastPCA results for\n",
    "dropped_arsvd_2 = arsvd_2.drop([16, 20, 22])\n",
    "dropped_arsvd_3 = arsvd_3.drop([16, 20, 22])\n",
    "dropped_arsvd_5 = arsvd_5.drop([16, 20, 22])\n",
    "dropped_arsvd_10 = arsvd_10.drop([16, 20, 22])\n",
    "\n",
    "dropped_arsvd_avgs = [dropped_arsvd_2[\"Value\"].mean(), dropped_arsvd_3[\"Value\"].mean(), dropped_arsvd_5[\"Value\"].mean(), dropped_arsvd_10[\"Value\"].mean()]\n",
    "fastpca_avgs = [fastpca_2[\"Value\"].mean(), fastpca_3[\"Value\"].mean(), fastpca_5[\"Value\"].mean(), fastpca_10[\"Value\"].mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averages (no datasets dropped)\n",
    "figure_dir = \"../output/figures\"\n",
    "plt.plot(k_vals, arsvd_avgs, label = \"ARSVD\")\n",
    "plt.plot(k_vals, fastpca_avgs, label = \"FastPCA\")\n",
    "plt.xlabel(\"Value of k\")\n",
    "plt.ylabel(\"Proportion of Variance Explained\")\n",
    "plt.title(\"Average Performance on Simulated Data (All)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f\"{figure_dir}/all_arsvd_fastpca_avg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averages (datasets dropped)\n",
    "\n",
    "plt.plot(k_vals, dropped_arsvd_avgs, label = \"ARSVD\")\n",
    "plt.plot(k_vals, fastpca_avgs, label = \"FastPCA\")\n",
    "plt.xlabel(\"Value of k\")\n",
    "plt.ylabel(\"Proportion of Variance Explained\")\n",
    "plt.title(\"Average Performance on Simulated Data (Dropped)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f\"{figure_dir}/dropped_arsvd_fastpca_avg.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averages by sparsity\n",
    "sparsity_cutoff = 0.85\n",
    "sparse_matrices = G_df[G_df[\"Proportion Zeros\"] >= sparsity_cutoff]\n",
    "sparse_matrices = sparse_matrices.reset_index()\n",
    "sparse_matrices[[\"n\", \"sequence_length\", \"mut_rate\"]] = pd.DataFrame(sparse_matrices[\"Parameters\"].tolist())\n",
    "\n",
    "sparse_matrices.drop(columns = ['index', 'Parameters', 'Shape of G'], inplace = True)\n",
    "\n",
    "arsvd2_sparse = pd.merge(sparse_matrices, arsvd_2, on = [\"n\", \"sequence_length\", \"mut_rate\"])\n",
    "arsvd3_sparse = pd.merge(sparse_matrices, arsvd_3, on = [\"n\", \"sequence_length\", \"mut_rate\"])\n",
    "arsvd5_sparse = pd.merge(sparse_matrices, arsvd_5, on = [\"n\", \"sequence_length\", \"mut_rate\"])\n",
    "arsvd10_sparse = pd.merge(sparse_matrices, arsvd_10, on = [\"n\", \"sequence_length\", \"mut_rate\"])\n",
    "\n",
    "dense_matrices = G_df[G_df[\"Proportion Zeros\"] < sparsity_cutoff]\n",
    "dense_matrices = dense_matrices.reset_index()\n",
    "dense_matrices[[\"n\", \"sequence_length\", \"mut_rate\"]] = pd.DataFrame(dense_matrices[\"Parameters\"].tolist())\n",
    "\n",
    "dense_matrices.drop(columns = ['index', 'Parameters', 'Shape of G'], inplace = True)\n",
    "\n",
    "arsvd2_dense = pd.merge(dense_matrices, arsvd_2, on = [\"n\", \"sequence_length\", \"mut_rate\"])\n",
    "arsvd3_dense = pd.merge(dense_matrices, arsvd_3, on = [\"n\", \"sequence_length\", \"mut_rate\"])\n",
    "arsvd5_dense = pd.merge(dense_matrices, arsvd_5, on = [\"n\", \"sequence_length\", \"mut_rate\"])\n",
    "arsvd10_dense = pd.merge(dense_matrices, arsvd_10, on = [\"n\", \"sequence_length\", \"mut_rate\"])\n",
    "\n",
    "fastpca2_sparse = pd.merge(sparse_matrices, fastpca_2, on = [\"n\", \"sequence_length\", \"mut_rate\"])\n",
    "fastpca3_sparse = pd.merge(sparse_matrices, fastpca_3, on = [\"n\", \"sequence_length\", \"mut_rate\"])\n",
    "fastpca5_sparse = pd.merge(sparse_matrices, fastpca_5, on = [\"n\", \"sequence_length\", \"mut_rate\"])\n",
    "fastpca10_sparse = pd.merge(sparse_matrices, fastpca_10, on = [\"n\", \"sequence_length\", \"mut_rate\"])\n",
    "\n",
    "fastpca2_dense = pd.merge(dense_matrices, fastpca_2, on = [\"n\", \"sequence_length\", \"mut_rate\"])\n",
    "fastpca3_dense = pd.merge(dense_matrices, fastpca_3, on = [\"n\", \"sequence_length\", \"mut_rate\"])\n",
    "fastpca5_dense = pd.merge(dense_matrices, fastpca_5, on = [\"n\", \"sequence_length\", \"mut_rate\"])\n",
    "fastpca10_dense = pd.merge(dense_matrices, fastpca_10, on = [\"n\", \"sequence_length\", \"mut_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averages (sparse)\n",
    "\n",
    "# Averaging performance across all datasets per k\n",
    "arsvd_sparse_avgs = [arsvd2_sparse[\"Value\"].mean(), arsvd3_sparse[\"Value\"].mean(), arsvd5_sparse[\"Value\"].mean(), arsvd10_sparse[\"Value\"].mean()]\n",
    "fastpca_sparse_avgs = [fastpca2_sparse[\"Value\"].mean(), fastpca3_sparse[\"Value\"].mean(), fastpca5_sparse[\"Value\"].mean(), fastpca10_sparse[\"Value\"].mean()]\n",
    "\n",
    "plt.plot(k_vals, arsvd_sparse_avgs, label = \"ARSVD\")\n",
    "plt.plot(k_vals, fastpca_sparse_avgs, label = \"FastPCA\")\n",
    "plt.xlabel(\"Value of k\")\n",
    "plt.ylabel(\"Proportion of Variance Explained\")\n",
    "plt.title(\"Average Performance on Simulated Data (Sparse)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f\"{figure_dir}/sparse_arsvd_fastpca_avg.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averages (dense)\n",
    "\n",
    "# Averaging performance across all datasets per k\n",
    "arsvd_dense_avgs = [arsvd2_dense[\"Value\"].mean(), arsvd3_dense[\"Value\"].mean(), arsvd5_dense[\"Value\"].mean(), arsvd10_dense[\"Value\"].mean()]\n",
    "fastpca_dense_avgs = [fastpca2_dense[\"Value\"].mean(), fastpca3_dense[\"Value\"].mean(), fastpca5_dense[\"Value\"].mean(), fastpca10_dense[\"Value\"].mean()]\n",
    "\n",
    "plt.plot(k_vals, arsvd_dense_avgs, label = \"ARSVD\")\n",
    "plt.plot(k_vals, fastpca_dense_avgs, label = \"FastPCA\")\n",
    "plt.xlabel(\"Value of k\")\n",
    "plt.ylabel(\"Proportion of Variance Explained\")\n",
    "plt.title(\"Average Performance on Simulated Data (Dense)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(f\"{figure_dir}/dense_arsvd_fastpca_avg.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
